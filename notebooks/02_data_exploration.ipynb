{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastf1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure matplotlib for inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastf1.Cache.enable_cache('../data')  # Cache data in the 'data' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_race_data(years, race_names):\n",
    "    all_results = []\n",
    "    all_laps = []\n",
    "    all_qualy = []\n",
    "    all_qualy_laps = []\n",
    "    all_car_data = []\n",
    "    all_positions = []\n",
    "\n",
    "    for year in years:\n",
    "        for race_name in race_names:\n",
    "            try:\n",
    "                fastf1.Cache.offline_mode(enabled=True)\n",
    "                # Load race session\n",
    "                session = fastf1.get_session(year, race_name, 'R')\n",
    "\n",
    "                if session is None:\n",
    "                    print(f\"Session not found for {year} {race_name}\")\n",
    "                    continue\n",
    "\n",
    "                session.load()\n",
    "\n",
    "                results = session.results\n",
    "                results['Year'] = year\n",
    "                results['RaceName'] = race_name\n",
    "                all_results.append(results)\n",
    "\n",
    "                # Collect lap data\n",
    "                laps = session.laps\n",
    "                weather_data = laps.get_weather_data()\n",
    "                laps = laps.reset_index(drop=True)\n",
    "                weather_data = weather_data.reset_index(drop=True)\n",
    "                joined = pd.concat([laps, weather_data.loc[:, ~(weather_data.columns == 'Time')]], axis=1)\n",
    "                laps['Year'] = year\n",
    "                laps['RaceName'] = race_name\n",
    "                all_laps.append(joined)\n",
    "\n",
    "                # Collect telemetry and positional data\n",
    "                car_data = session.car_data\n",
    "                pos_data = session.pos_data\n",
    "\n",
    "                # Collect telemetry data for each driver\n",
    "                for driver in session.drivers:\n",
    "                    # Collect telemetry for the driver\n",
    "                    if driver in car_data:\n",
    "                        driver_car_data = car_data[driver]\n",
    "                        driver_car_data['DriverNumber'] = driver\n",
    "                        driver_car_data['Year'] = year\n",
    "                        driver_car_data['RaceName'] = race_name\n",
    "                        all_car_data.append(driver_car_data)\n",
    "\n",
    "                    # Collect positional data for the driver\n",
    "                    if driver in pos_data:\n",
    "                        driver_pos_data = pos_data[driver]\n",
    "                        driver_pos_data['DriverNumber'] = driver\n",
    "                        driver_pos_data['Year'] = year\n",
    "                        driver_pos_data['RaceName'] = race_name\n",
    "                        all_positions.append(driver_pos_data)\n",
    "\n",
    "                # Load qualifying session\n",
    "                qualy_session = fastf1.get_session(year, race_name, 'Q')\n",
    "                qualy_session.load()\n",
    "                qualy_results = qualy_session.results\n",
    "                qualy_results['Year'] = year\n",
    "                qualy_results['RaceName'] = race_name\n",
    "                all_qualy.append(qualy_results[['DriverNumber', 'Position']].rename(columns={'Position': 'QualiPosition'}))\n",
    "\n",
    "                # Collect lap data for qualifying\n",
    "                qualy_laps = qualy_session.laps\n",
    "                weather_data = qualy_laps.get_weather_data()\n",
    "                qualy_laps = qualy_laps.reset_index(drop=True)\n",
    "                weather_data = weather_data.reset_index(drop=True)\n",
    "                joined = pd.concat([qualy_laps, weather_data.loc[:, ~(weather_data.columns == 'Time')]], axis=1)\n",
    "                joined['Year'] = year\n",
    "                joined['RaceName'] = race_name\n",
    "                all_qualy_laps.append(joined)\n",
    "\n",
    "                print(f\"Loaded data for {race_name} {year}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {race_name} {year}: {e}\")\n",
    "\n",
    "    # Concatenate all results and laps dataframes\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    all_laps_df = pd.concat(all_laps, ignore_index=True)\n",
    "    all_qualy_df = pd.concat(all_qualy, ignore_index=True)\n",
    "    all_qualy_laps_df = pd.concat(all_qualy_laps, ignore_index=True)\n",
    "    all_car_data = pd.concat(all_car_data, ignore_index=True)\n",
    "    all_positions_df = pd.concat(all_positions, ignore_index=True)\n",
    "\n",
    "    return all_results_df, all_laps_df, all_qualy_df, all_qualy_laps_df, all_car_data, all_positions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2019, 2025))\n",
    "race_names = list(range(1, 25))\n",
    "# years = [2020]\n",
    "# race_names = [1,2,3]\n",
    "# Collect the data\n",
    "results, laps, qualy, qualylaps, car_data, positions = collect_race_data(years, race_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('../data/historical_results.csv', index=False)\n",
    "laps.to_csv('../data/historical_laps.csv', index=False)\n",
    "qualy.to_csv('../data/historical_qualy.csv', index=False)\n",
    "car_data.to_csv('../data/historical_car_data.csv', index=False)\n",
    "positions.to_csv('../data/historical_positions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_features = pd.DataFrame()\n",
    "\n",
    "# Average lap time\n",
    "# Merge lap times with driver results\n",
    "lap_times = results.merge(laps, on='DriverNumber')\n",
    "\n",
    "# Calculate average lap time per driver\n",
    "avg_lap_times = lap_times.groupby('DriverNumber')['LapTime'].mean().reset_index()\n",
    "avg_lap_times.rename(columns={'LapTime': 'AvgLapTime'}, inplace=True)\n",
    "\n",
    "# Merge with results\n",
    "results_df = results[['DriverNumber', 'FullName', 'TeamName', 'Position', 'Points', 'Status']]\n",
    "results_with_features = pd.merge(results_df, avg_lap_times, on='DriverNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top speed\n",
    "# Calculate top speed per driver\n",
    "# Note: Ensure 'SpeedST' or 'Speed' is available in your laps data\n",
    "if 'SpeedST' in laps.columns:\n",
    "    speed_column = 'SpeedST'\n",
    "elif 'Speed' in laps.columns:\n",
    "    speed_column = 'Speed'\n",
    "else:\n",
    "    print(\"Speed data not available in laps dataframe.\")\n",
    "    speed_column = None\n",
    "\n",
    "if speed_column:\n",
    "    top_speeds = laps.groupby('DriverNumber')[speed_column].max().reset_index()\n",
    "    top_speeds.rename(columns={speed_column: 'TopSpeed'}, inplace=True)\n",
    "\n",
    "    # Merge with results_with_features\n",
    "    results_with_features = pd.merge(results_with_features, top_speeds, on='DriverNumber', how='left')\n",
    "else:\n",
    "    print(\"Top Speed feature cannot be added due to missing speed data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate telemetry features for each driver\n",
    "car_telemetry_features = car_data.groupby('DriverNumber').agg({\n",
    "    'Speed': ['mean', 'max'],  # Calculate both average and max speed\n",
    "    'RPM': 'mean',             # Average RPM\n",
    "    'Throttle': 'mean',        # Average throttle usage\n",
    "    'Brake': 'mean'            # Average brake usage\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "# Rename the columns properly to avoid conflicts\n",
    "car_telemetry_features.columns = ['DriverNumber', 'AvgSpeed', 'MaxSpeed', 'AvgRPM', 'AvgThrottle', 'AvgBrake']\n",
    "\n",
    "# Merge telemetry features with the main dataset\n",
    "results_with_features = pd.merge(results_with_features, car_telemetry_features, on='DriverNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out invalid position data\n",
    "valid_positions = positions[(positions['X'] != 0) & (positions['Y'] != 0) & (positions['Status'] != 'OffTrack')]\n",
    "\n",
    "# Aggregate positional features for each driver\n",
    "positional_features = valid_positions.groupby('DriverNumber').agg({\n",
    "    'X': 'mean',  # Average X position on track\n",
    "    'Y': 'mean',  # Average Y position on track\n",
    "    'Z': 'mean',  # Average Z position (altitude, if relevant)\n",
    "    'Time': 'count'  # Number of valid position samples\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "positional_features.rename(columns={\n",
    "    'X': 'AvgXPosition',\n",
    "    'Y': 'AvgYPosition',\n",
    "    'Z': 'AvgZPosition',  # Optional, if Z is relevant\n",
    "    'Time': 'NumPositionSamples'  # Total number of valid position data points\n",
    "}, inplace=True)\n",
    "\n",
    "# Merge positional features with the main dataset\n",
    "results_with_features = pd.merge(results_with_features, positional_features, on='DriverNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify pit laps where 'PitInTime' is not null\n",
    "pit_laps = laps[laps['PitInTime'].notnull()]\n",
    "# Count pit stops per driver\n",
    "pit_stop_counts = pit_laps.groupby('DriverNumber').size().reset_index(name='NumPitStops')\n",
    "\n",
    "# Merge with results_with_features\n",
    "results_with_features = pd.merge(results_with_features, pit_stop_counts, on='DriverNumber', how='left')\n",
    "# Fill NaN values with 0 for drivers without pit stops\n",
    "results_with_features['NumPitStops'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge qualifying position into results_with_features\n",
    "results_with_features = pd.merge(results_with_features, qualy, on='DriverNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tyre compound information\n",
    "tyre_compounds = laps[['DriverNumber', 'Compound']]\n",
    "\n",
    "# Determine the most used compound for each driver\n",
    "most_used_compound = tyre_compounds.groupby('DriverNumber')['Compound'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "most_used_compound.rename(columns={'Compound': 'MostUsedCompound'}, inplace=True)\n",
    "\n",
    "# One-hot encode the tyre compounds\n",
    "tyre_dummies = pd.get_dummies(most_used_compound['MostUsedCompound'], prefix='Tyre')\n",
    "\n",
    "# Merge with driver data\n",
    "most_used_compound = pd.concat([most_used_compound, tyre_dummies], axis=1)\n",
    "\n",
    "# Merge with results_with_features\n",
    "results_with_features = pd.merge(results_with_features, most_used_compound.drop('MostUsedCompound', axis=1), on='DriverNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weather data\n",
    "weather_data = laps[['DriverNumber', 'AirTemp', 'Humidity', 'Pressure', 'TrackTemp', 'WindDirection', 'WindSpeed']]\n",
    "\n",
    "# Calculate the min, max, and standard deviation for weather conditions per driver\n",
    "weather_variability = weather_data.groupby('DriverNumber').agg({\n",
    "    'AirTemp': ['mean', 'min', 'max', 'std'],\n",
    "    'Humidity': ['mean', 'min', 'max', 'std'],\n",
    "    'Pressure': ['mean', 'min', 'max', 'std'],\n",
    "    'TrackTemp': ['mean', 'min', 'max', 'std'],\n",
    "    'WindDirection': ['mean', 'min', 'max', 'std'],\n",
    "    'WindSpeed': ['mean', 'min', 'max', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "weather_variability.columns = ['DriverNumber', 'AvgAirTemp', 'MinAirTemp', 'MaxAirTemp', 'StdAirTemp',\n",
    "                               'AvgHumidity', 'MinHumidity', 'MaxHumidity', 'StdHumidity',\n",
    "                               'AvgPressure', 'MinPressure', 'MaxPressure', 'StdPressure',\n",
    "                               'AvgTrackTemp', 'MinTrackTemp', 'MaxTrackTemp', 'StdTrackTemp',\n",
    "                               'AvgWindDirection', 'MinWindDirection', 'MaxWindDirection', 'StdWindDirection',\n",
    "                               'AvgWindSpeed', 'MinWindSpeed', 'MaxWindSpeed', 'StdWindSpeed']\n",
    "\n",
    "# Calculate the rate of change for AirTemp and WindSpeed over the race\n",
    "laps['AirTempChange'] = laps['AirTemp'].diff()\n",
    "laps['WindSpeedChange'] = laps['WindSpeed'].diff()\n",
    "\n",
    "# Aggregate rate of change statistics per driver\n",
    "weather_rate_of_change = laps.groupby('DriverNumber').agg({\n",
    "    'AirTempChange': ['mean', 'std'],  # Mean and std deviation of temperature change\n",
    "    'WindSpeedChange': ['mean', 'std']  # Mean and std deviation of wind speed change\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the column names\n",
    "weather_rate_of_change.columns = ['DriverNumber', 'AvgAirTempChange', 'StdAirTempChange',\n",
    "                                  'AvgWindSpeedChange', 'StdWindSpeedChange']\n",
    "\n",
    "\n",
    "# Merge weather variability features into results_with_features\n",
    "results_with_features = pd.merge(results_with_features, weather_variability, on='DriverNumber', how='left')\n",
    "\n",
    "# Merge weather rate of change features into results_with_features\n",
    "results_with_features = pd.merge(results_with_features, weather_rate_of_change, on='DriverNumber', how='left')\n",
    "\n",
    "results_with_features.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average pit stop duration per driver\n",
    "pit_stop_durations = pit_laps.groupby('DriverNumber')['PitOutTime'].mean().reset_index()\n",
    "pit_stop_durations.rename(columns={'PitOutTime': 'AvgPitStopDuration'}, inplace=True)\n",
    "\n",
    "# Merge with results_with_features\n",
    "results_with_features = pd.merge(results_with_features, pit_stop_durations, on='DriverNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "laps['Sector1Time'] = pd.to_timedelta(laps['Sector1Time']).dt.total_seconds()\n",
    "laps['Sector2Time'] = pd.to_timedelta(laps['Sector2Time']).dt.total_seconds()\n",
    "laps['Sector3Time'] = pd.to_timedelta(laps['Sector3Time']).dt.total_seconds()\n",
    "\n",
    "# Aggregate sector times for each driver (e.g., average sector time per driver)\n",
    "sector_times = laps.groupby('DriverNumber').agg({\n",
    "    'Sector1Time': 'mean',  # Average sector 1 time\n",
    "    'Sector2Time': 'mean',  # Average sector 2 time\n",
    "    'Sector3Time': 'mean'   # Average sector 3 time\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "sector_times.rename(columns={\n",
    "    'Sector1Time': 'AvgSector1Time',\n",
    "    'Sector2Time': 'AvgSector2Time',\n",
    "    'Sector3Time': 'AvgSector3Time'\n",
    "}, inplace=True)\n",
    "\n",
    "# Merge sector times with the main results_with_features DataFrame\n",
    "results_with_features = pd.merge(results_with_features, sector_times, on='DriverNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualylaps['Sector1Time'] = pd.to_timedelta(qualylaps['Sector1Time']).dt.total_seconds()\n",
    "qualylaps['Sector2Time'] = pd.to_timedelta(qualylaps['Sector2Time']).dt.total_seconds()\n",
    "qualylaps['Sector3Time'] = pd.to_timedelta(qualylaps['Sector3Time']).dt.total_seconds()\n",
    "qualylaps['LapTime'] = pd.to_timedelta(qualylaps['LapTime']).dt.total_seconds()\n",
    "\n",
    "# Extract the fastest qualifying lap and best sector times for each driver\n",
    "fastest_qualy_lap = qualylaps.groupby('DriverNumber').agg({\n",
    "    'LapTime': 'min',              # Fastest lap time during qualifying\n",
    "    'Sector1Time': 'min',          # Fastest sector 1 time\n",
    "    'Sector2Time': 'min',          # Fastest sector 2 time\n",
    "    'Sector3Time': 'min'           # Fastest sector 3 time\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "fastest_qualy_lap.rename(columns={\n",
    "    'LapTime': 'FastestQualyLapTime',\n",
    "    'Sector1Time': 'FastestQualySector1Time',\n",
    "    'Sector2Time': 'FastestQualySector2Time',\n",
    "    'Sector3Time': 'FastestQualySector3Time'\n",
    "}, inplace=True)\n",
    "\n",
    "# Merge fastest qualifying lap and sector times with the main dataset\n",
    "results_with_features = pd.merge(results_with_features, fastest_qualy_lap, on='DriverNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average qualifying lap time for each driver\n",
    "average_qualy_lap = qualylaps.groupby('DriverNumber').agg({\n",
    "    'LapTime': 'mean'  # Average lap time during qualifying\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the column\n",
    "average_qualy_lap.rename(columns={'LapTime': 'AvgQualyLapTime'}, inplace=True)\n",
    "\n",
    "# Merge average qualifying lap time with the main dataset\n",
    "results_with_features = pd.merge(results_with_features, average_qualy_lap, on='DriverNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timedelta features to seconds\n",
    "time_features = ['AvgLapTime', 'AvgPitStopDuration']\n",
    "for feature in time_features:\n",
    "    results_with_features[feature] = pd.to_timedelta(results_with_features[feature]).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicated weather columns with '_y' suffix\n",
    "results_with_features.drop(columns=[col for col in results_with_features.columns if '_y' in col], inplace=True)\n",
    "\n",
    "# Rename the columns with '_x' suffix to remove it\n",
    "results_with_features.columns = results_with_features.columns.str.replace('_x', '')\n",
    "\n",
    "results_with_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List continuous features to scale\n",
    "continuous_features = ['AvgLapTime', 'TopSpeed', 'NumPitStops', 'QualiPosition',\n",
    "                       'AvgThrottle', 'AvgBrake', 'AvgSpeed', 'MaxSpeed',\n",
    "                       'AvgRPM', 'AvgXPosition', 'AvgYPosition', 'AvgZPosition',\n",
    "                       'AvgPitStopDuration', 'NumPositionSamples',\n",
    "                       'AvgSector1Time', 'AvgSector2Time', 'AvgSector3Time',\n",
    "                       'FastestQualyLapTime', 'FastestQualySector1Time',\n",
    "                       'FastestQualySector2Time', 'FastestQualySector3Time',\n",
    "                       'AvgQualyLapTime',\n",
    "                       'MinAirTemp', 'MaxAirTemp', 'StdAirTemp',\n",
    "                       'MinHumidity', 'MaxHumidity', 'StdHumidity',\n",
    "                       'MinPressure', 'MaxPressure', 'StdPressure',\n",
    "                       'MinTrackTemp', 'MaxTrackTemp', 'StdTrackTemp',\n",
    "                       'MinWindDirection', 'MaxWindDirection', 'StdWindDirection',\n",
    "                       'MinWindSpeed', 'MaxWindSpeed', 'StdWindSpeed',\n",
    "                       'AvgAirTempChange', 'StdAirTempChange',\n",
    "                       'AvgWindSpeedChange', 'StdWindSpeedChange']\n",
    "\n",
    "# Select features and target variable\n",
    "features = continuous_features + list(tyre_dummies.columns)\n",
    "target = 'Position'\n",
    "\n",
    "# Handle missing values if any\n",
    "results_with_features.dropna(subset=features + [target], inplace=True)\n",
    "\n",
    "\n",
    "# Define X and y\n",
    "X = results_with_features[features]\n",
    "y = results_with_features[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale only the continuous features\n",
    "X_train_continuous = scaler.fit_transform(X_train[continuous_features])\n",
    "X_test_continuous = scaler.transform(X_test[continuous_features])\n",
    "\n",
    "# Convert scaled features back to DataFrame for easy concatenation with categorical features\n",
    "X_train_continuous_df = pd.DataFrame(X_train_continuous, columns=continuous_features, index=X_train.index)\n",
    "X_test_continuous_df = pd.DataFrame(X_test_continuous, columns=continuous_features, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the scaled continuous features with the categorical (unscaled) features\n",
    "X_train_final = pd.concat([X_train_continuous_df, X_train.drop(columns=continuous_features)], axis=1)\n",
    "X_test_final = pd.concat([X_test_continuous_df, X_test.drop(columns=continuous_features)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, '../models/f1_model_2.pkl')\n",
    "\n",
    "# Save the scaler if you used one\n",
    "joblib.dump(scaler, '../models/scaler_2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "feature_names = features\n",
    "\n",
    "# Create a DataFrame\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(data=feature_importance_df, x='Importance', y='Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(residuals, bins=20, kde=True)\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residual (Actual - Predicted Position)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check alignment\n",
    "mismatches = results_with_features[results_with_features['QualiPosition'].isnull()]\n",
    "print(f\"Number of drivers without qualifying position: {len(mismatches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variance in weather features\n",
    "weather_features = ['AvgAirTemp', 'AvgHumidity', 'AvgPressure', 'AvgTrackTemp', 'AvgWindDirection', 'AvgWindSpeed']\n",
    "weather_variability = results_with_features[weather_features].std()\n",
    "print(weather_variability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all object columns to numeric, coercing errors\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == 'object':\n",
    "        X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "        X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "\n",
    "# You may also need to handle NaN values after conversion, if any\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Mean Absolute Error: {mae_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances_xgb = xgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame\n",
    "feature_importance_xgb_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances_xgb})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_xgb_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(data=feature_importance_xgb_df, x='Importance', y='Feature')\n",
    "plt.title('XGBoost Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred_xgb\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(residuals, bins=20, kde=True)\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residual (Actual - Predicted Position)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize the LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "mae_lgb = mean_absolute_error(y_test, y_pred_lgb)\n",
    "print(f\"LightGBM Mean Absolute Error: {mae_lgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances_lgb = lgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame\n",
    "feature_importance_lgb_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances_lgb})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_lgb_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(data=feature_importance_lgb_df, x='Importance', y='Feature')\n",
    "plt.title('LightGBM Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred_lgb\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(residuals, bins=20, kde=True)\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residual (Actual - Predicted Position)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Hyperparameter Tuning for Random Forest\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(2, 10),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=20, cv=5, scoring='neg_mean_absolute_error', random_state=42)\n",
    "\n",
    "# Perform the search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_score_)\n",
    "print(random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column names in results_with_features\n",
    "print(results_with_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the year and race name available in some variables\n",
    "results_with_features['Year'] = 2018  # Replace race_year with the actual year value\n",
    "results_with_features['RaceName'] = 1  # Replace race_name with the actual race name or number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter your race data for Monza 2024\n",
    "monza_2024_data = results_with_features[\n",
    "    (results_with_features['Year'] == 2018) & (results_with_features['RaceName'] == 1)\n",
    "]\n",
    "\n",
    "# Select only the features that the model was trained on\n",
    "X_1_2024 = monza_2024_data[features]  # 'features' is the list you used for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the trained RandomForest model (or LightGBM, XGBoost)\n",
    "predicted_positions = random_search.best_estimator_.predict(X_1_2024)\n",
    "\n",
    "# Add the predictions to the dataframe for easier inspection\n",
    "monza_2024_data['PredictedPosition'] = predicted_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the predicted positions along with driver names and actual positions (if available)\n",
    "print(monza_2024_data[['DriverNumber', 'FullName', 'TeamName', 'PredictedPosition']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If actual race positions are available, compare predicted vs actual\n",
    "monza_2024_data['ActualPosition'] = monza_2024_data['Position']  # Assuming 'Position' column has the true results\n",
    "\n",
    "# Compare predicted vs actual\n",
    "comparison = monza_2024_data[['DriverNumber', 'FullName', 'TeamName', 'PredictedPosition', 'ActualPosition']]\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate lap-level data to one row per driver per race\n",
    "monza_2024_agg = monza_2024_data.groupby(['DriverNumber', 'FullName', 'TeamName']).agg({\n",
    "    'AvgLapTime': 'mean',\n",
    "    'TopSpeed': 'mean',\n",
    "    'AvgSpeed': 'mean',\n",
    "    'MaxSpeed': 'mean',\n",
    "    'AvgRPM': 'mean',\n",
    "    'AvgThrottle': 'mean',\n",
    "    'AvgBrake': 'mean',\n",
    "    'NumPitStops': 'mean',   # Example for aggregation, adjust as necessary\n",
    "    'QualiPosition': 'mean',\n",
    "    # Add other necessary feature aggregations here\n",
    "    'Position': 'first'  # Actual race position, using 'first' to get the original value\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features you used for training\n",
    "X_monza_2024_agg = monza_2024_agg[[\n",
    "    'AvgLapTime', 'TopSpeed', 'AvgSpeed', 'MaxSpeed', 'AvgRPM', 'AvgThrottle', 'AvgBrake', 'NumPitStops', 'QualiPosition'\n",
    "]]  # 'features' is the list of features used in training\n",
    "\n",
    "# Predict the driver positions\n",
    "predicted_positions = random_search.best_estimator_.predict(X_monza_2024_agg)\n",
    "\n",
    "# Add the predicted positions to the aggregated DataFrame\n",
    "monza_2024_agg['PredictedPosition'] = predicted_positions\n",
    "\n",
    "# Compare with actual positions\n",
    "comparison = monza_2024_agg[['DriverNumber', 'FullName', 'TeamName', 'PredictedPosition', 'Position']]\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Hyperparameter Tuning:\n",
    "# \t•\tBoth XGBoost and LightGBM are highly sensitive to hyperparameters. A grid search or random search across hyperparameters like learning_rate, max_depth, and n_estimators might reduce the MAE.\n",
    "# \t•\tHere’s how you could start tuning:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, scoring='neg_mean_absolute_error', cv=3)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "print(\"Best parameters for XGBoost:\", grid_search_xgb.best_params_)\n",
    "print(\"Best MAE for XGBoost:\", -grid_search_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Hyperparameter Tuning:\n",
    "# \t•\tBoth XGBoost and LightGBM are highly sensitive to hyperparameters. A grid search or random search across hyperparameters like learning_rate, max_depth, and n_estimators might reduce the MAE.\n",
    "# \t•\tHere’s how you could start tuning:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid for XGBoost\n",
    "param_grid_lgb = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search_lgb = GridSearchCV(estimator=lgb_model, param_grid=param_grid_lgb, scoring='neg_mean_absolute_error', cv=3)\n",
    "grid_search_lgb.fit(X_train, y_train)\n",
    "print(\"Best parameters for LightGBM:\", grid_search_lgb.best_params_)\n",
    "print(\"Best MAE for LightGBM:\", -grid_search_lgb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize the XGBoost and LightGBM models with the tuned parameters\n",
    "xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the models\n",
    "xgb_model.fit(X_train, y_train)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# Averaging the predictions\n",
    "y_pred_avg = (y_pred_xgb + y_pred_lgb) / 2\n",
    "\n",
    "# Evaluate the averaged predictions\n",
    "mae_avg = mean_absolute_error(y_test, y_pred_avg)\n",
    "print(f\"Model Averaging Mean Absolute Error: {mae_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Get predictions on the training data (for stacking)\n",
    "y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "y_train_pred_lgb = lgb_model.predict(X_train)\n",
    "\n",
    "# Stack the predictions together as new features for the meta-model\n",
    "stacked_train = np.column_stack((y_train_pred_xgb, y_train_pred_lgb))\n",
    "\n",
    "# Train the meta-model (Linear Regression)\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "# Get predictions from the base models on the test set\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "y_test_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# Stack the test set predictions\n",
    "stacked_test = np.column_stack((y_test_pred_xgb, y_test_pred_lgb))\n",
    "\n",
    "# Meta-model makes final predictions\n",
    "y_pred_stacked = meta_model.predict(stacked_test)\n",
    "\n",
    "# Evaluate the stacked model\n",
    "mae_stacked = mean_absolute_error(y_test, y_pred_stacked)\n",
    "print(f\"Model Stacking Mean Absolute Error: {mae_stacked}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid\n",
    "learning_rates = [0.01, 0.05, 0.1]\n",
    "max_depths = [3, 6, 9]\n",
    "n_estimators = [100, 200, 300]\n",
    "\n",
    "# Keep track of the best parameters and MAE\n",
    "best_mae = np.inf\n",
    "best_params = {}\n",
    "\n",
    "# Loop through all combinations of parameters\n",
    "for lr in learning_rates:\n",
    "    for depth in max_depths:\n",
    "        for n_est in n_estimators:\n",
    "            # Initialize and train the LightGBM model with current parameters\n",
    "            lgb_model = lgb.LGBMRegressor(n_estimators=n_est, learning_rate=lr, max_depth=depth, random_state=42)\n",
    "            lgb_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict and evaluate\n",
    "            y_pred_lgb = lgb_model.predict(X_test)\n",
    "            mae_lgb = mean_absolute_error(y_test, y_pred_lgb)\n",
    "            print(f\"Parameters: learning_rate={lr}, max_depth={depth}, n_estimators={n_est} -> MAE: {mae_lgb}\")\n",
    "\n",
    "            # Update best parameters if MAE improves\n",
    "            if mae_lgb < best_mae:\n",
    "                best_mae = mae_lgb\n",
    "                best_params = {'learning_rate': lr, 'max_depth': depth, 'n_estimators': n_est}\n",
    "\n",
    "print(f\"Best Parameters: {best_params} with MAE: {best_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Initialize the LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and MAE\n",
    "best_params = grid_search.best_params_\n",
    "best_mae = -grid_search.best_score_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Mean Absolute Error: {best_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "fastf1.Cache.offline_mode(enabled=False)\n",
    "def collect_and_store_race_data(years, race_names):\n",
    "    for year in years:\n",
    "        for race_name in race_names:\n",
    "            try:\n",
    "                session = fastf1.get_session(year, race_name, 'R')\n",
    "\n",
    "                if session is None:\n",
    "                    print(f\"Session not found for {year} {race_name}\")\n",
    "                    return\n",
    "\n",
    "                session.load()\n",
    "                print(f\"Race data for {race_name} {year} loaded.\")\n",
    "\n",
    "                qualy_session = fastf1.get_session(year, race_name, 'Q')\n",
    "                qualy_session.load()\n",
    "                print(f\"Qualifying data for {race_name} {year} loaded.\")\n",
    "\n",
    "                print(f\"Saved data for {race_name} {year}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {race_name} {year}: {e}\")\n",
    "\n",
    "years = [2017]\n",
    "race_names = [1,2]\n",
    "collect_and_store_race_data(years, race_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
